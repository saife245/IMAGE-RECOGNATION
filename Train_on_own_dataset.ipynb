{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "- Train the model on your own dataset\n",
    "- use Efficientnet-b5 as backbone\n",
    "- And Imagenet as weight(you may use noisy-student also)\n",
    "- Train on Tpu for 40 epocs\n",
    "- Use Gridmasking and Mixup technique\n",
    "- Also may apply cutout/cutmix/Augmix/mixup only/etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/kaggle-efficientnet-repo/efficientnet-1.0.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.6/site-packages (from efficientnet==1.0.0) (0.16.2)\r\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /opt/conda/lib/python3.6/site-packages (from efficientnet==1.0.0) (1.0.8)\r\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (2.6.1)\r\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (1.1.1)\r\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (2.4)\r\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (3.0.3)\r\n",
      "Requirement already satisfied: pillow>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (5.4.1)\r\n",
      "Requirement already satisfied: scipy>=0.19.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (1.4.1)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0) (2.10.0)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0) (1.18.1)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0) (4.4.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (1.1.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (2.4.6)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (2.8.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0) (1.14.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (45.2.0.post20200210)\r\n",
      "Installing collected packages: efficientnet\r\n",
      "Successfully installed efficientnet-1.0.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/kaggle-efficientnet-repo/efficientnet-1.0.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "def transform(image, inv_mat, image_shape):\n",
    "    h, w, c = image_shape\n",
    "    cx, cy = w//2, h//2\n",
    "    new_xs = tf.repeat( tf.range(-cx, cx, 1), h)\n",
    "    new_ys = tf.tile( tf.range(-cy, cy, 1), [w])\n",
    "    new_zs = tf.ones([h*w], dtype=tf.int32)\n",
    "    old_coords = tf.matmul(inv_mat, tf.cast(tf.stack([new_xs, new_ys, new_zs]), tf.float32))\n",
    "    old_coords_x, old_coords_y = tf.round(old_coords[0, :] + w//2), tf.round(old_coords[1, :] + h//2)\n",
    "    clip_mask_x = tf.logical_or(old_coords_x<0, old_coords_x>w-1)\n",
    "    clip_mask_y = tf.logical_or(old_coords_y<0, old_coords_y>h-1)\n",
    "    clip_mask = tf.logical_or(clip_mask_x, clip_mask_y)\n",
    "    old_coords_x = tf.boolean_mask(old_coords_x, tf.logical_not(clip_mask))\n",
    "    old_coords_y = tf.boolean_mask(old_coords_y, tf.logical_not(clip_mask))\n",
    "    new_coords_x = tf.boolean_mask(new_xs+cx, tf.logical_not(clip_mask))\n",
    "    new_coords_y = tf.boolean_mask(new_ys+cy, tf.logical_not(clip_mask))\n",
    "    old_coords = tf.cast(tf.stack([old_coords_y, old_coords_x]), tf.int32)\n",
    "    new_coords = tf.cast(tf.stack([new_coords_y, new_coords_x]), tf.int64)\n",
    "    rotated_image_values = tf.gather_nd(image, tf.transpose(old_coords))\n",
    "    rotated_image_channel = list()\n",
    "    for i in range(c):\n",
    "        vals = rotated_image_values[:,i]\n",
    "        sparse_channel = tf.SparseTensor(tf.transpose(new_coords), vals, [h, w])\n",
    "        rotated_image_channel.append(tf.sparse.to_dense(sparse_channel, default_value=0, validate_indices=False))\n",
    "    return tf.transpose(tf.stack(rotated_image_channel), [1,2,0])\n",
    "\n",
    "def random_rotate(image, angle, image_shape):\n",
    "    def get_rotation_mat_inv(angle):\n",
    "        # transform to radian\n",
    "        angle = math.pi * angle / 180\n",
    "        cos_val = tf.math.cos(angle)\n",
    "        sin_val = tf.math.sin(angle)\n",
    "        one = tf.constant([1], tf.float32)\n",
    "        zero = tf.constant([0], tf.float32)\n",
    "        rot_mat_inv = tf.concat([cos_val, sin_val, zero, -sin_val, cos_val, zero, zero, zero, one], axis=0)\n",
    "        rot_mat_inv = tf.reshape(rot_mat_inv, [3,3])\n",
    "        return rot_mat_inv\n",
    "    angle = float(angle) * tf.random.normal([1],dtype='float32')\n",
    "    rot_mat_inv = get_rotation_mat_inv(angle)\n",
    "    return transform(image, rot_mat_inv, image_shape)\n",
    "\n",
    "\n",
    "def GridMask(image_height, image_width, d1, d2, rotate_angle=1, ratio=0.5):\n",
    "    h, w = image_height, image_width\n",
    "    hh = int(np.ceil(np.sqrt(h*h+w*w)))\n",
    "    hh = hh+1 if hh%2==1 else hh\n",
    "    d = tf.random.uniform(shape=[], minval=d1, maxval=d2, dtype=tf.int32)\n",
    "    l = tf.cast(tf.cast(d,tf.float32)*ratio+0.5, tf.int32)\n",
    "\n",
    "    st_h = tf.random.uniform(shape=[], minval=0, maxval=d, dtype=tf.int32)\n",
    "    st_w = tf.random.uniform(shape=[], minval=0, maxval=d, dtype=tf.int32)\n",
    "\n",
    "    y_ranges = tf.range(-1 * d + st_h, -1 * d + st_h + l)\n",
    "    x_ranges = tf.range(-1 * d + st_w, -1 * d + st_w + l)\n",
    "\n",
    "    for i in range(0, hh//d+1):\n",
    "        s1 = i * d + st_h\n",
    "        s2 = i * d + st_w\n",
    "        y_ranges = tf.concat([y_ranges, tf.range(s1,s1+l)], axis=0)\n",
    "        x_ranges = tf.concat([x_ranges, tf.range(s2,s2+l)], axis=0)\n",
    "\n",
    "    x_clip_mask = tf.logical_or(x_ranges < 0 , x_ranges > hh-1)\n",
    "    y_clip_mask = tf.logical_or(y_ranges < 0 , y_ranges > hh-1)\n",
    "    clip_mask = tf.logical_or(x_clip_mask, y_clip_mask)\n",
    "\n",
    "    x_ranges = tf.boolean_mask(x_ranges, tf.logical_not(clip_mask))\n",
    "    y_ranges = tf.boolean_mask(y_ranges, tf.logical_not(clip_mask))\n",
    "\n",
    "    hh_ranges = tf.tile(tf.range(0,hh), [tf.cast(tf.reduce_sum(tf.ones_like(x_ranges)), tf.int32)])\n",
    "    x_ranges = tf.repeat(x_ranges, hh)\n",
    "    y_ranges = tf.repeat(y_ranges, hh)\n",
    "\n",
    "    y_hh_indices = tf.transpose(tf.stack([y_ranges, hh_ranges]))\n",
    "    x_hh_indices = tf.transpose(tf.stack([hh_ranges, x_ranges]))\n",
    "\n",
    "    y_mask_sparse = tf.SparseTensor(tf.cast(y_hh_indices, tf.int64),  tf.zeros_like(y_ranges), [hh, hh])\n",
    "    y_mask = tf.sparse.to_dense(y_mask_sparse, 1, False)\n",
    "\n",
    "    x_mask_sparse = tf.SparseTensor(tf.cast(x_hh_indices, tf.int64), tf.zeros_like(x_ranges), [hh, hh])\n",
    "    x_mask = tf.sparse.to_dense(x_mask_sparse, 1, False)\n",
    "\n",
    "    mask = tf.expand_dims( tf.clip_by_value(x_mask + y_mask, 0, 1), axis=-1)\n",
    "\n",
    "    mask = random_rotate(mask, rotate_angle, [hh, hh, 1])\n",
    "    mask = tf.image.crop_to_bounding_box(mask, (hh-h)//2, (hh-w)//2, image_height, image_width)\n",
    "\n",
    "    return mask\n",
    "\n",
    "def apply_grid_mask(image, image_shape):\n",
    "    AugParams = {\n",
    "        'd1' : 100,\n",
    "        'd2': 160,\n",
    "        'rotate' : 45,\n",
    "        'ratio' : 0.3\n",
    "    }\n",
    "    mask = GridMask(image_shape[0], image_shape[1], AugParams['d1'], AugParams['d2'], AugParams['rotate'], AugParams['ratio'])\n",
    "    if image_shape[-1] == 3:\n",
    "        mask = tf.concat([mask, mask, mask], axis=-1)\n",
    "    return image * tf.cast(mask, tf.float32)\n",
    "\n",
    "\n",
    "def gridmask(img_batch, label_batch, batch_size):\n",
    "    return apply_grid_mask(img_batch, (160, 256, 3)), label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  ['10.0.0.2:8470']\n",
      "REPLICAS:  8\n",
      "Using backbone efficientnet-b5 and weights imagenet\n",
      "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b5_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "115515392/115515256 [==============================] - 4s 0us/step\n",
      "Training on 160672 samples. Each epochs requires 1255 steps\n",
      "Train for 1255 steps\n",
      "Epoch 1/40\n",
      "1255/1255 [==============================] - 422s 336ms/step - loss: 6.0900 - categorical_accuracy: 0.0815 - top_k_categorical_accuracy: 0.2051 - val_loss: 1.9880 - val_categorical_accuracy: 0.5762 - val_top_k_categorical_accuracy: 0.8819\n",
      "Epoch 2/40\n",
      "1255/1255 [==============================] - 257s 205ms/step - loss: 3.7096 - categorical_accuracy: 0.5150 - top_k_categorical_accuracy: 0.7798 - val_loss: 0.5185 - val_categorical_accuracy: 0.8948 - val_top_k_categorical_accuracy: 0.9847\n",
      "Epoch 3/40\n",
      "1255/1255 [==============================] - 257s 205ms/step - loss: 2.9910 - categorical_accuracy: 0.6799 - top_k_categorical_accuracy: 0.8726 - val_loss: 0.3178 - val_categorical_accuracy: 0.9349 - val_top_k_categorical_accuracy: 0.9926\n",
      "Epoch 4/40\n",
      "1255/1255 [==============================] - 257s 205ms/step - loss: 2.1363 - categorical_accuracy: 0.7498 - top_k_categorical_accuracy: 0.9176 - val_loss: 0.1764 - val_categorical_accuracy: 0.9657 - val_top_k_categorical_accuracy: 0.9964\n",
      "Epoch 7/40\n",
      "1255/1255 [==============================] - 257s 205ms/step - loss: 1.9541 - categorical_accuracy: 0.7591 - top_k_categorical_accuracy: 0.9301 - val_loss: 0.1679 - val_categorical_accuracy: 0.9695 - val_top_k_categorical_accuracy: 0.9972\n",
      "Epoch 8/40\n",
      "1255/1255 [==============================] - 256s 204ms/step - loss: 1.8236 - categorical_accuracy: 0.7637 - top_k_categorical_accuracy: 0.9358 - val_loss: 0.1467 - val_categorical_accuracy: 0.9717 - val_top_k_categorical_accuracy: 0.9972\n",
      "Epoch 9/40\n",
      "1255/1255 [==============================] - 257s 205ms/step - loss: 1.7235 - categorical_accuracy: 0.7646 - top_k_categorical_accuracy: 0.9398 - val_loss: 0.1305 - val_categorical_accuracy: 0.9711 - val_top_k_categorical_accuracy: 0.9978\n",
      "Epoch 10/40\n",
      "1255/1255 [==============================] - 255s 203ms/step - loss: 1.6318 - categorical_accuracy: 0.7688 - top_k_categorical_accuracy: 0.9453 - val_loss: 0.1271 - val_categorical_accuracy: 0.9733 - val_top_k_categorical_accuracy: 0.9980\n",
      "Epoch 11/40\n",
      "1255/1255 [==============================] - 256s 204ms/step - loss: 1.5594 - categorical_accuracy: 0.7734 - top_k_categorical_accuracy: 0.9519 - val_loss: 0.1174 - val_categorical_accuracy: 0.9745 - val_top_k_categorical_accuracy: 0.9978\n",
      "Epoch 12/40\n",
      "1255/1255 [==============================] - 257s 205ms/step - loss: 1.2301 - categorical_accuracy: 0.7843 - top_k_categorical_accuracy: 0.9749 - val_loss: 0.0928 - val_categorical_accuracy: 0.9797 - val_top_k_categorical_accuracy: 0.9978\n",
      "Epoch 21/40\n",
      "1255/1255 [==============================] - 258s 205ms/step - loss: 1.1111 - categorical_accuracy: 0.8004 - top_k_categorical_accuracy: 0.9817 - val_loss: 0.0862 - val_categorical_accuracy: 0.9799 - val_top_k_categorical_accuracy: 0.9982\n",
      "Epoch 28/40\n",
      "1255/1255 [==============================] - 259s 206ms/step - loss: 1.0893 - categorical_accuracy: 0.7989 - top_k_categorical_accuracy: 0.9795 - val_loss: 0.0790 - val_categorical_accuracy: 0.9815 - val_top_k_categorical_accuracy: 0.9986\n",
      "Epoch 30/40\n",
      "1255/1255 [==============================] - 257s 205ms/step - loss: 1.0636 - categorical_accuracy: 0.8050 - top_k_categorical_accuracy: 0.9834 - val_loss: 0.0765 - val_categorical_accuracy: 0.9841 - val_top_k_categorical_accuracy: 0.9988\n",
      "Epoch 32/40\n",
      "1255/1255 [==============================] - 258s 205ms/step - loss: 1.0433 - categorical_accuracy: 0.7976 - top_k_categorical_accuracy: 0.9839 - val_loss: 0.0788 - val_categorical_accuracy: 0.9827 - val_top_k_categorical_accuracy: 0.9988\n",
      "Epoch 34/40\n",
      "1255/1255 [==============================] - 258s 206ms/step - loss: 1.0277 - categorical_accuracy: 0.8032 - top_k_categorical_accuracy: 0.9854 - val_loss: 0.0759 - val_categorical_accuracy: 0.9847 - val_top_k_categorical_accuracy: 0.9988\n",
      "Epoch 36/40\n",
      "1255/1255 [==============================] - 255s 204ms/step - loss: 1.0097 - categorical_accuracy: 0.8054 - top_k_categorical_accuracy: 0.9848 - val_loss: 0.0694 - val_categorical_accuracy: 0.9849 - val_top_k_categorical_accuracy: 0.9992\n",
      "Epoch 38/40\n",
      "1255/1255 [==============================] - 258s 206ms/step - loss: 0.9954 - categorical_accuracy: 0.8073 - top_k_categorical_accuracy: 0.9875 - val_loss: 0.0693 - val_categorical_accuracy: 0.9833 - val_top_k_categorical_accuracy: 0.9986\n",
      "Epoch 40/40\n",
      " 202/1255 [===>..........................] - ETA: 3:02 - loss: 0.9815 - categorical_accuracy: 0.8097 - top_k_categorical_accuracy: 0.9845"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "\n",
    "from tensorflow.keras import layers as L\n",
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "\n",
    "def normalize(image):\n",
    "  # https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/main.py#L325-L326\n",
    "  # https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_builder.py#L31-L32\n",
    "  image -= tf.constant([0.485 * 255, 0.456 * 255, 0.406 * 255])  # RGB\n",
    "  image /= tf.constant([0.229 * 255, 0.224 * 255, 0.225 * 255])  # RGB\n",
    "  return image\n",
    "\n",
    "\n",
    "def get_model(input_size, backbone='efficientnet-b0', weights='imagenet', tta=False):\n",
    "  print(f'Using backbone {backbone} and weights {weights}')\n",
    "  x = L.Input(shape=input_size, name='imgs', dtype='float32')\n",
    "  y = normalize(x)\n",
    "  if backbone.startswith('efficientnet'):\n",
    "    model_fn = getattr(efn, f'EfficientNetB{backbone[-1]}')\n",
    "\n",
    "  y = model_fn(input_shape=input_size, weights=weights, include_top=False)(y)\n",
    "  y = L.GlobalAveragePooling2D()(y)\n",
    "  y = L.Dropout(0.2)(y)\n",
    "  # 1292 of 1295 are present\n",
    "  y = L.Dense(1292, activation='softmax')(y)\n",
    "  model = tf.keras.Model(x, y)\n",
    "\n",
    "  if tta:\n",
    "    assert False, 'This does not make sense yet'\n",
    "    x_flip = tf.reverse(x, [2])  # 'NHWC'\n",
    "    y_tta = tf.add(model(x), model(x_flip)) / 2.0\n",
    "    tta_model = tf.keras.Model(x, y_tta)\n",
    "    return model, tta_model\n",
    "\n",
    "  return model\n",
    "\n",
    "#Applying the Mixup\n",
    "def mixup(img_batch, label_batch, batch_size):\n",
    "  # https://github.com/tensorpack/tensorpack/blob/master/examples/ResNet/cifar10-preact18-mixup.py\n",
    "  weight = tf.random.uniform([batch_size])\n",
    "  x_weight = tf.reshape(weight, [batch_size, 1, 1, 1])\n",
    "  y_weight = tf.reshape(weight, [batch_size, 1])\n",
    "  index = tf.random.shuffle(tf.range(batch_size, dtype=tf.int32))\n",
    "  x1, x2 = img_batch, tf.gather(img_batch, index)\n",
    "  img_batch = x1 * x_weight + x2 * (1. - x_weight)\n",
    "  y1, y2 = label_batch, tf.gather(label_batch, index)\n",
    "  label_batch = y1 * y_weight + y2 * (1. - y_weight)\n",
    "  return img_batch, label_batch\n",
    "\n",
    "\n",
    "def get_strategy():\n",
    "  # Detect hardware, return appropriate distribution strategy\n",
    "  try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "  except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "  if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "  else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "  print('REPLICAS: ', strategy.num_replicas_in_sync)\n",
    "  return strategy\n",
    "\n",
    "\n",
    "def one_hot(image, label):\n",
    "  label = tf.one_hot(label, 1292)\n",
    "  return image, label\n",
    "\n",
    "\n",
    "def read_tfrecords(example, input_size):\n",
    "  features = {\n",
    "      'img': tf.io.FixedLenFeature([], tf.string),\n",
    "      'image_id': tf.io.FixedLenFeature([], tf.int64),\n",
    "      'grapheme_root': tf.io.FixedLenFeature([], tf.int64),\n",
    "      'vowel_diacritic': tf.io.FixedLenFeature([], tf.int64),\n",
    "      'consonant_diacritic': tf.io.FixedLenFeature([], tf.int64),\n",
    "      'unique_tuple': tf.io.FixedLenFeature([], tf.int64),\n",
    "  }\n",
    "  example = tf.io.parse_single_example(example, features)\n",
    "  img = tf.image.decode_image(example['img'])\n",
    "  img = tf.reshape(img, input_size + (1, ))\n",
    "  img = tf.cast(img, tf.float32)\n",
    "  # grayscale -> RGB\n",
    "  img = tf.repeat(img, 3, -1)\n",
    "\n",
    "  # image_id = tf.cast(example['image_id'], tf.int32)\n",
    "  # grapheme_root = tf.cast(example['grapheme_root'], tf.int32)\n",
    "  # vowel_diacritic = tf.cast(example['vowel_diacritic'], tf.int32)\n",
    "  # consonant_diacritic = tf.cast(example['consonant_diacritic'], tf.int32)\n",
    "  unique_tuple = tf.cast(example['unique_tuple'], tf.int32)\n",
    "  return img, unique_tuple\n",
    "\n",
    "\n",
    "def main():\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--model_id', type=int, default=0)\n",
    "  parser.add_argument('--seed', type=int, default=123)\n",
    "  parser.add_argument('--lr', type=float, default=1e-4)\n",
    "  parser.add_argument('--input_size', type=str, default='160,256')\n",
    "  parser.add_argument('--batch_size', type=int, default=128)\n",
    "  parser.add_argument('--epochs', type=int, default=40)\n",
    "  parser.add_argument('--backbone', type=str, default='efficientnet-b5')\n",
    "  parser.add_argument('--weights', type=str, default='imagenet')\n",
    "  args, _ = parser.parse_known_args()\n",
    "\n",
    "  args.input_size = tuple(int(x) for x in args.input_size.split(','))\n",
    "  np.random.seed(args.seed)\n",
    "  tf.random.set_seed(args.seed)\n",
    "\n",
    "  # build the model\n",
    "  strategy = get_strategy()\n",
    "  with strategy.scope():\n",
    "    model = get_model(input_size=args.input_size + (3, ), backbone=args.backbone,\n",
    "        weights=args.weights)\n",
    "\n",
    "  model.compile(optimizer=Adam(lr=args.lr, beta_1=0.9, beta_2=0.999, decay=0.00001),\n",
    "                loss=categorical_crossentropy,\n",
    "                metrics=[categorical_accuracy, top_k_categorical_accuracy])\n",
    "  # print(model.summary())\n",
    "\n",
    "  # create the training and validation datasets\n",
    "  ds_path = KaggleDatasets().get_gcs_path('bengali-tfrecords-v010')\n",
    "  train_fns = tf.io.gfile.glob(os.path.join(ds_path, 'records/train*.tfrec'))\n",
    "  train_ds = tf.data.TFRecordDataset(train_fns)\n",
    "  train_ds = train_ds.map(lambda e: read_tfrecords(e, args.input_size))\n",
    "  train_ds = train_ds.repeat().batch(args.batch_size)\n",
    "  train_ds = train_ds.map(one_hot)\n",
    "  train_ds = train_ds.map(lambda a,b: gridmask(a,b, args.batch_size))\n",
    "  train_ds = train_ds.map(lambda a, b: mixup(a, b, args.batch_size))\n",
    "\n",
    "  val_fns = tf.io.gfile.glob(os.path.join(ds_path, 'records/val*.tfrec'))\n",
    "  val_ds = tf.data.TFRecordDataset(val_fns)\n",
    "  val_ds = val_ds.map(lambda e: read_tfrecords(e, args.input_size))\n",
    "  val_ds = val_ds.batch(args.batch_size)\n",
    "  val_ds = val_ds.map(one_hot)\n",
    "\n",
    "  # train\n",
    "  num_train_samples = sum(int(fn.split('_')[2]) for fn in train_fns)\n",
    "  # num_val_samples = sum(int(fn.split('_')[2]) for fn in val_fns)\n",
    "  steps_per_epoch = num_train_samples // args.batch_size\n",
    "  print(f'Training on {num_train_samples} samples. Each epochs requires {steps_per_epoch} steps')\n",
    "  h = model.fit(train_ds, steps_per_epoch=steps_per_epoch, epochs=args.epochs, verbose=1,\n",
    "      validation_data=val_ds)\n",
    "  print(h)\n",
    "  weight_fn = 'model-%04d.h5' % args.model_id\n",
    "  model.save_weights(weight_fn)\n",
    "  print(f'Saved weights to: {weight_fn}')\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook__.ipynb  model-0000.h5\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
